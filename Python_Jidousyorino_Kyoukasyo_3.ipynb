{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 「シゴトがはかどるPython自動処理の教科書」 クジラ飛行机 著 (つづき)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webブラウザの自動化／スクレイピング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Requestsモジュールを使ってダウンロードしよう**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023/10/20 09:46:25\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://api.aoikujira.com/time/get.php'\n",
    "result = requests.get(url)\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok= True\n",
      "text= 2023/10/20 09:50:44\n",
      "status_code= 200\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://api.aoikujira.com/time/get.php'\n",
    "result = requests.get(url)\n",
    "print('ok=', result.ok)\n",
    "if result.ok:\n",
    "    print('text=', result.text)\n",
    "    print('status_code=', result.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**画像ファイルをダウンロードしてみよう**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://uta.pw/shodou/img/3/3.png'\n",
    "res = requests.get(url)\n",
    "if not res.ok:\n",
    "    print('失敗:', res.status_code)\n",
    "    quit()\n",
    "with open('gyudon.png', 'wb') as fp:\n",
    "    fp.write(res.content)\n",
    "print('ok.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for文と組み合わせて連続で画像をダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save: img_chap4/1.png\n",
      "save: img_chap4/2.png\n",
      "save: img_chap4/3.png\n",
      "save: img_chap4/4.png\n",
      "save: img_chap4/5.png\n",
      "save: img_chap4/6.png\n",
      "save: img_chap4/7.png\n",
      "save: img_chap4/8.png\n",
      "save: img_chap4/9.png\n",
      "save: img_chap4/10.png\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os, time\n",
    "\n",
    "save_dir = 'img_chap4/'\n",
    "base_url = 'https://uta.pw/shodou/img/{0}/{1}.png'\n",
    "\n",
    "def download_all():\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "    for id in range(1, 11):\n",
    "        download_file(id)\n",
    "        time.sleep(1)\n",
    "\n",
    "def download_file(id):\n",
    "    url = base_url.format(id%31, id)\n",
    "    save_file = save_dir + str(id) + '.png'\n",
    "    res = requests.get(url)\n",
    "    if not res.ok:\n",
    "        print('失敗:', res.status_code)\n",
    "        return\n",
    "    with open(save_file, 'wb') as fp:\n",
    "        fp.write(res.content)\n",
    "    print('save:', save_file)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    download_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**スクレイピング -- BeautifulSoup を使おう**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "魚について\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open('html_chap4/fish.html', encoding='utf-8') as fp:\n",
    "    html_str = fp.read()\n",
    "\n",
    "soup = BeautifulSoup(html_str, 'html5lib')\n",
    "title = soup.find('title')\n",
    "print(title.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "複数の要素を検索するときは、find_allメソッド"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "カツオ\n",
      "ウナギ\n",
      "サケ\n"
     ]
    }
   ],
   "source": [
    "fishes = soup.find_all('h2')\n",
    "for e in fishes:\n",
    "    print(e.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ウナギの値段を調べる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12,800円/kg\n"
     ]
    }
   ],
   "source": [
    "for h2 in soup.find_all('h2'):\n",
    "    if h2.string == 'ウナギ':\n",
    "        for e in h2.next_siblings:\n",
    "            if e.name == 'p':\n",
    "                if e['class'][0] == 'price':\n",
    "                    print(e.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12,800円/kg\n"
     ]
    }
   ],
   "source": [
    "unagi = soup.find(id='eel')\n",
    "unagi_price = unagi.find(class_ = 'price')\n",
    "print(unagi_price.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12,800円/kg\n"
     ]
    }
   ],
   "source": [
    "p = soup.select('div#eel > p.price')\n",
    "print(p[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "魚と解説と値段を列挙する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "カツオ 大型肉食魚。鉄分が多く味の個性が強い。 2,980円/kg\n",
      "ウナギ 海で産卵し川に戻る。蒲焼きで食べる。 12,800円/kg\n",
      "サケ 川で産卵し海で過ごす。脂のりが良い。 1,890円/kg\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open('html_chap4/fish.html', encoding='utf-8') as fp:\n",
    "    html_str = fp.read()\n",
    "soup = BeautifulSoup(html_str, 'html5lib')\n",
    "\n",
    "res = []\n",
    "div_list = soup.select('#fishes > div')\n",
    "for div in div_list:\n",
    "    fish = div.h2.text\n",
    "    desc = div.select('.desc')[0].text\n",
    "    price = div.select('.price')[0].text\n",
    "    print(fish, desc, price)\n",
    "    res.append([fish, desc, price])\n",
    "\n",
    "import csv\n",
    "with open('html_chap4/fish.csv', 'wt', encoding='sjis', newline='') as fp:\n",
    "    csv.writer(fp).writerows(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ページ内のリンクを集めて一気にダウンロードしよう**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 名作作品のページのHTMLを取得\n",
    "2. BeautifulSoupで解析して画像のURLを取得\n",
    "3. 取得したURLの画像を連続でダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, requests, urllib\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "target_url = 'https://uta.pw/shodou/index.php?master'\n",
    "save_dir = 'img_chap4/meisaku/'\n",
    "\n",
    "# ダウンロードのメイン処理\n",
    "def download_images():\n",
    "    html = requests.get(target_url).text\n",
    "    urls = get_image_urls(html)\n",
    "    go_download(urls)\n",
    "\n",
    "# HTMLから画像のURL一覧を取得\n",
    "def get_image_urls(html):\n",
    "    soup = BeautifulSoup(html, 'html5lib')\n",
    "    res = []\n",
    "    for img in soup.find_all('imag'):\n",
    "        src = img['src'] # 相対パス\n",
    "        url = urllib.parse.urljoin(target_url, src) #絶対パスに変換\n",
    "        print('img.src=', url)\n",
    "        res.append(url)\n",
    "    return res\n",
    "\n",
    "# 連続でURL一覧をダウンロード\n",
    "def go_download(urls):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "    for url in urls:\n",
    "        fname = os.path.basename(url)\n",
    "        save_file = save_dir + fname\n",
    "        r = requests.get(url)\n",
    "        with open(save_file, 'wb') as fp:\n",
    "            fp.write(r.content)\n",
    "            print('save:', save_file)\n",
    "        time.sleep(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    download_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**相対URLを絶対URLに変換するurljoin関数**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```絶対URL = urllib.parse.urljoin(基本URL, 相対URL)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://example.com/a/b/hoge.jpg'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.parse import urljoin\n",
    "\n",
    "urljoin('https://example.com/a/b/c.html', 'hoge.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://example.com/a/fuga.jpg'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urljoin('https://example.com/a/b/c.html', '../fuga.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://example.com/img/foo.jpg'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urljoin('https://example.com/a/b/c.html', '../../img/foo.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://example.com/logo.jpg'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urljoin('https://example.com/a/b/c.html', '/logo.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**サイトリンクをたどって丸ごと資料を取得しよう**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python公式ドキュメント、チュートリアルページのダウンロード"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "処理の手順\n",
    "\n",
    "1. 基本となるページを取得(https://doc.python.org/ja/3/tutorial/index.html)\n",
    "2. リンクされているURLを列挙して以下繰り返す\n",
    "3. そのリンクがチュートリアル外なら何もしない\n",
    "4. そのリンクがすでに取得済みであれば何もしない\n",
    "5. HTMLをファイルに保存\n",
    "6. 取得したページからリンクされているURLを列挙して3.に戻る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save: ./pydoc_tutorial/index.html\n",
      "save: ./pydoc_tutorial/appetite.html\n",
      "save: ./pydoc_tutorial/interpreter.html\n",
      "save: ./pydoc_tutorial/introduction.html\n",
      "save: ./pydoc_tutorial/controlflow.html\n",
      "save: ./pydoc_tutorial/datastructures.html\n",
      "save: ./pydoc_tutorial/modules.html\n",
      "save: ./pydoc_tutorial/inputoutput.html\n",
      "save: ./pydoc_tutorial/errors.html\n",
      "save: ./pydoc_tutorial/classes.html\n",
      "save: ./pydoc_tutorial/stdlib.html\n",
      "save: ./pydoc_tutorial/stdlib2.html\n",
      "save: ./pydoc_tutorial/venv.html\n",
      "save: ./pydoc_tutorial/whatnow.html\n",
      "save: ./pydoc_tutorial/interactive.html\n",
      "save: ./pydoc_tutorial/floatingpoint.html\n",
      "save: ./pydoc_tutorial/appendix.html\n"
     ]
    }
   ],
   "source": [
    "import requests, urllib, os, time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "save_dir = './pydoc_tutorial/'\n",
    "top_page = 'https://doc.python.org/ja/3/tutorial/index.html'\n",
    "check_url = 'https://doc.python.org/ja/3/tutorial/'\n",
    "visited ={}\n",
    "\n",
    "def get_page(url): # 指定されたURLのページを取得する\n",
    "    if check_url not in url:\n",
    "        return\n",
    "    if url in visited:\n",
    "        return\n",
    "    visited[url] = True # 訪問済にする\n",
    "    res = requests.get(url)\n",
    "    res.encoding = res.apparent_encoding\n",
    "    html = res.text\n",
    "    fname = save_dir + os.path.basename(url)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "    with open(fname, 'wt', encoding='utf-8') as f:\n",
    "        f.write(html)\n",
    "        print('save:',fname)\n",
    "    time.sleep(1)\n",
    "    soup = BeautifulSoup(html, 'html5lib')\n",
    "    for a in soup.find_all('a'):\n",
    "        a_url = urllib.parse.urljoin(url, a['href'])\n",
    "        a_url = urllib.parse.urldefrag(a_url)[0]\n",
    "        get_page(a_url)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    get_page(top_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Webブラウザを自動操縦しよう**  \n",
    "**--ライブラリのインストール編**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SeleniumとChromeDriverのインストール"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chromeを起動してPythonのページを表示するプログラムの動作確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://python.org')\n",
    "time.sleep(30)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "任意のページのスクリーンショットを撮影するプログラム"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https:ojizou003.com')\n",
    "driver.save_screenshot('./img_chap4/ojizou003.png')\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Webブラウザを自動操縦しよう**  \n",
    "**--基本マスター編**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作詞掲示板の作品ページから作品タイトルと作者を取得するプログラム"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "人はいさ 心も知らず ふるさとは 花ぞ昔の 香に匂ひける --- 百人一首 作\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "url = 'https://uta.pw/sakusibbs/post.php?mml_id=35'\n",
    "driver.get(url)\n",
    "e = driver.find_element_by_class_name('posttitle')\n",
    "print(e.text)\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "掲示板のトップページから名作アーカイブというリンクを探して、そのページを表示するプログラム"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get('https://uta.pw/sakusibbs/')\n",
    "link = driver.find_element_by_link_text('名作アーカイブ')\n",
    "link.click()\n",
    "time.sleep(30)\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**フォーム送信**  \n",
    "**--Google検索を自動化しよう**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get('https://www.google.co.jp/webhp?rls=ig')\n",
    "el = driver.find_element_by_name('q')\n",
    "el.send_keys('Pythonの教科書')\n",
    "el.submit()\n",
    "\n",
    "time.sleep(30)\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ヘッドレスモードで起動しよう**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "■ サーバーメンテナンス後のテストの巻\n",
      " ∟ https://uta.pw/sakusibbs/post.php?mml_id=1384\n",
      "■ どこまでも\n",
      " ∟ https://uta.pw/sakusibbs/post.php?mml_id=1232\n",
      "■ テストとはテスト\n",
      " ∟ https://uta.pw/sakusibbs/post.php?mml_id=421\n",
      "■ ヨモギ\n",
      " ∟ https://uta.pw/sakusibbs/post.php?mml_id=347\n",
      "■ すたーとふろむ風呂\n",
      " ∟ https://uta.pw/sakusibbs/post.php?mml_id=346\n",
      "■ 取り外す\n",
      " ∟ https://uta.pw/sakusibbs/post.php?mml_id=272\n",
      "■ 危険な試験\n",
      " ∟ https://uta.pw/sakusibbs/post.php?mml_id=270\n",
      "■ 数え歌\n",
      " ∟ https://uta.pw/sakusibbs/post.php?mml_id=245\n",
      "■ 爽やかな風の中で\n",
      " ∟ https://uta.pw/sakusibbs/post.php?mml_id=206\n",
      "■ リトマス紙と私\n",
      " ∟ https://uta.pw/sakusibbs/post.php?mml_id=141\n",
      "■ サラダよ皿に乗れ\n",
      " ∟ https://uta.pw/sakusibbs/post.php?mml_id=127\n",
      "■ ずーっとるーむ\n",
      " ∟ https://uta.pw/sakusibbs/post.php?mml_id=124\n",
      "■ チキンカレーだけが世界\n",
      " ∟ https://uta.pw/sakusibbs/post.php?mml_id=121\n",
      "■ 夕焼けと船\n",
      " ∟ https://uta.pw/sakusibbs/post.php?mml_id=118\n",
      "■ 逃亡\n",
      " ∟ https://uta.pw/sakusibbs/post.php?mml_id=116\n",
      "■ ゆうひ\n",
      " ∟ https://uta.pw/sakusibbs/post.php?mml_id=110\n",
      "■ おもちゃ箱ガラガラ行進曲の歌詞\n",
      " ∟ https://uta.pw/sakusibbs/post.php?mml_id=108\n",
      "■ 海辺でゆったり\n",
      " ∟ https://uta.pw/sakusibbs/post.php?mml_id=105\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "driver.get('https://uta.pw/sakusibbs/users.php?user_id=1')\n",
    "a_list = driver.find_elements_by_css_selector('ul#mmlist li a')\n",
    "for a in a_list:\n",
    "    print('■',a.text)\n",
    "    print(' ∟',a.get_attribute('href'))\n",
    "    time.sleep(1)\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ページ全体のスクリーンショットを撮る方法**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_size: 1019 2638\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "url = 'https://python.org'\n",
    "save_file = 'img_chap4/screenshot_full.png'\n",
    "\n",
    "# メイン処理\n",
    "def screenshot_full(url, save_file):\n",
    "    w, h = get_page_size(url)\n",
    "    screenshot_size(url, save_file, w, h)\n",
    "\n",
    "# ページの幅と高さを取得する\n",
    "def get_page_size(url):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    w = driver.execute_script(\n",
    "        'return document.body.scrollWidth;'\n",
    "    )\n",
    "    h = driver.execute_script(\n",
    "        'return document.body.scrollHeight;'\n",
    "    )\n",
    "    driver.close()\n",
    "    print('page_size:', w, h)\n",
    "    return(w, h)\n",
    "\n",
    "# 指定サイズでページを保存\n",
    "def screenshot_size(url, save_file, w, h):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    win_size = 'window-size=' + str(w) + ',' + str(h)\n",
    "    options.add_argument(win_size)\n",
    "    cap_driver = webdriver.Chrome(options = options)\n",
    "    cap_driver.get(url)\n",
    "    cap_driver.save_screenshot(save_file)\n",
    "    cap_driver.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    screenshot_full(url, save_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**会員制Webサイトからデータをダウンロード**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作詞掲示板にログインして、マイページからCSVファイルをダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import os, time\n",
    "\n",
    "login_url = 'https://uta.pw/sakusibbs/users.php?action=login'\n",
    "user_id, password = ('おぢぞう', 'ipCU12ySxl')\n",
    "save_dir = 'C:/Users/sinis/programming/08_Python/Python_Jidousyorino_kyoukasyo/csv_chap4/'\n",
    "save_file = save_dir + 'list.csv'\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option('prefs', {'download.default_directory': save_dir})\n",
    "\n",
    "#メイン処理\n",
    "def login_download():\n",
    "    driver = webdriver.Chrome(options = options)\n",
    "    try_login(driver)\n",
    "    link_click(driver, 'マイページ')\n",
    "    time.sleep(3)\n",
    "    link_click(driver, 'ダウンロード')\n",
    "    for i in range(30):\n",
    "        if os.path.exists(save_file):\n",
    "            break\n",
    "        time.sleep(1)\n",
    "    driver.close()\n",
    "\n",
    "# ログイン処理\n",
    "def try_login(driver):\n",
    "    driver.get(login_url)\n",
    "    user = driver.find_element_by_name('username_mmlbbs6')\n",
    "    user.send_keys(user_id)\n",
    "    pwd = driver.find_element_by_name('password_mmlbbs6')\n",
    "    pwd.send_keys(password)\n",
    "    pwd.submit()\n",
    "\n",
    "# ラベルを指定してリンクを検索しクリックする\n",
    "def link_click(driver, label):\n",
    "    a = driver.find_element_by_partial_link_text(label)\n",
    "    a.click()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    login_download()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jidou",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
